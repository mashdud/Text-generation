{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uY5C2hPR_1wl"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOWTHPO9AxZW",
    "outputId": "6d83adb4-5dd1-449b-bd7b-d038d41b0a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "it was seven o’clock of a very warm evening in the seeonee hills when\n",
      "father wolf woke up from his day’s rest, scratched himself, yawned, and\n",
      "spread out his paws one after the other to get rid of the sleepy feeling\n",
      "in their tips. mother wolf lay with her big gray nose dropped across her\n",
      "four tumbling, squealing cubs, and the moon shone into the mouth of the\n",
      "cave where they all lived. “augrh!” said father wolf. “it is time to\n",
      "hunt again.” he was going to spring down hill when a little shadow with\n",
      "a bushy tail crossed the threshold and whined: “good luck go with you, o\n",
      "chief of the wolves. and good luck and strong white teeth go with noble\n",
      "children that they may never forget the hungry in this world.”\n",
      "\n",
      "it was the jackal--tabaqui, the dish-licker--and the wolves of india\n",
      "despise tabaqui because he runs about making mischief, and telling\n",
      "tales, and eating rags and pieces of leather from the village\n",
      "rubbish-heaps. but they are afraid of him too, because tabaqui, more\n",
      "than anyone else in t\n"
     ]
    }
   ],
   "source": [
    "#LOAD T\n",
    "#Save notepad as UTF-8 (select from dropdown during saving)\n",
    "filename = \"/content/drive/MyDrive/236-0.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "print(raw_text[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciu1EW84ej2i"
   },
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IeFx6nlJImhE"
   },
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "#Remove numbers\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bu6ykwMUIyIR"
   },
   "outputs": [],
   "source": [
    "#How many total characters do we have in our training text?\n",
    "chars = sorted(list(set(raw_text))) #List of every character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmiDZA-8LTHt",
    "outputId": "861546eb-cb6a-4a88-9c37-3921970071c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '%',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CDx1QHmcI3gj"
   },
   "outputs": [],
   "source": [
    "#Character sequences must be encoded as integers. \n",
    "#Each unique character will be assigned an integer value. \n",
    "#Create a dictionary of characters mapped to integer values\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWjwrOmoLgTA",
    "outputId": "045f421d-8deb-4c27-969b-44be223e2930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '/': 11,\n",
       " ':': 12,\n",
       " ';': 13,\n",
       " '?': 14,\n",
       " '@': 15,\n",
       " '[': 16,\n",
       " ']': 17,\n",
       " '`': 18,\n",
       " 'a': 19,\n",
       " 'b': 20,\n",
       " 'c': 21,\n",
       " 'd': 22,\n",
       " 'e': 23,\n",
       " 'f': 24,\n",
       " 'g': 25,\n",
       " 'h': 26,\n",
       " 'i': 27,\n",
       " 'j': 28,\n",
       " 'k': 29,\n",
       " 'l': 30,\n",
       " 'm': 31,\n",
       " 'n': 32,\n",
       " 'o': 33,\n",
       " 'p': 34,\n",
       " 'q': 35,\n",
       " 'r': 36,\n",
       " 's': 37,\n",
       " 't': 38,\n",
       " 'u': 39,\n",
       " 'v': 40,\n",
       " 'w': 41,\n",
       " 'x': 42,\n",
       " 'y': 43,\n",
       " 'z': 44,\n",
       " '‘': 45,\n",
       " '’': 46,\n",
       " '“': 47,\n",
       " '”': 48,\n",
       " '\\ufeff': 49}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nFP3MNusI65_"
   },
   "outputs": [],
   "source": [
    "#Do the reverse so we can print our predictions in characters and not integers\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPKCEMjHI-2n",
    "outputId": "dd509826-7b52-4c72-bb6e-ee410753b323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the text; corpus length:  291499\n",
      "Total Vocab:  50\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxAzzhZjJNvj",
    "outputId": "68ab0f6d-ee9e-4bde-a206-dbe8a8ecb93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 29144\n"
     ]
    }
   ],
   "source": [
    "#Now that we have characters we can create input/output sequences for training\n",
    "#for LSTM input and output can be sequences... hence the term seq2seq\n",
    "\n",
    "\n",
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10   #Instead of moving 1 letter at a time, try skipping a few. \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values. The character that follows the sentence defined as X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGcHd3RIJXLB",
    "outputId": "36262b22-8ee6-4959-853e-29ab77080fcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-444fd40eaaf2>:15: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
      "<ipython-input-15-444fd40eaaf2>:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29144, 60, 50)\n",
      "(29144, 50)\n"
     ]
    }
   ],
   "source": [
    "#Have a look at sentences and next_chars to see the continuity...\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "\n",
    "#time steps = sequence length\n",
    "#features = numbers of characters in our vocab (n_vocab)\n",
    "#Vectorize all sentences: there are n_patterns sentences.\n",
    "#For each sentence we have n_vocab characters available for seq_length\n",
    "#Vectorization returns a vector for all sentences indicating the presence or absence \n",
    "#of a character. \n",
    "\n",
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#print(y[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMMx0JpvJ9Af",
    "outputId": "2170efef-46d4-429b-b813-266ed0e0e62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               91648     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Basic model with one LSTM\n",
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wlzeOadjKG6C"
   },
   "outputs": [],
   "source": [
    "# Deeper model woth 2 LSTM\n",
    "#To stack LSTM layers, we need to change the configuration of the prior \n",
    "#LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "#We can do this by setting the return_sequences argument on the layer to True \n",
    "#(defaults to False). This will return one output for each input time step and provide a 3D array.\n",
    "#Below is the same example as above with return_sequences=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a35vYYmrJut_",
    "outputId": "37a9a044-6da9-4701-fa23-5039c19768a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 60, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 128)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,682\n",
      "Trainable params: 229,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rjComcr4Jd4w"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ua92LqJOOGgy",
    "outputId": "833c6f56-d563-4de1-e69c-2d5b61048e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.6589\n",
      "Epoch 1: loss improved from inf to 2.65892, saving model to saved_weights/saved_weights-01-2.6589.hdf5\n",
      "228/228 [==============================] - 12s 18ms/step - loss: 2.6589\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.1791\n",
      "Epoch 2: loss improved from 2.65892 to 2.17911, saving model to saved_weights/saved_weights-02-2.1791.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 2.1791\n",
      "Epoch 3/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.9907\n",
      "Epoch 3: loss improved from 2.17911 to 1.99067, saving model to saved_weights/saved_weights-03-1.9907.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.9907\n",
      "Epoch 4/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.8689\n",
      "Epoch 4: loss improved from 1.99067 to 1.86905, saving model to saved_weights/saved_weights-04-1.8690.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.8690\n",
      "Epoch 5/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.7766\n",
      "Epoch 5: loss improved from 1.86905 to 1.77662, saving model to saved_weights/saved_weights-05-1.7766.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.7766\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.6987\n",
      "Epoch 6: loss improved from 1.77662 to 1.69868, saving model to saved_weights/saved_weights-06-1.6987.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.6987\n",
      "Epoch 7/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.6317\n",
      "Epoch 7: loss improved from 1.69868 to 1.63309, saving model to saved_weights/saved_weights-07-1.6331.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.6331\n",
      "Epoch 8/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.5801\n",
      "Epoch 8: loss improved from 1.63309 to 1.57974, saving model to saved_weights/saved_weights-08-1.5797.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.5797\n",
      "Epoch 9/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.5291\n",
      "Epoch 9: loss improved from 1.57974 to 1.52757, saving model to saved_weights/saved_weights-09-1.5276.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.5276\n",
      "Epoch 10/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.4825\n",
      "Epoch 10: loss improved from 1.52757 to 1.48337, saving model to saved_weights/saved_weights-10-1.4834.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.4834\n",
      "Epoch 11/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.4527\n",
      "Epoch 11: loss improved from 1.48337 to 1.45181, saving model to saved_weights/saved_weights-11-1.4518.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.4518\n",
      "Epoch 12/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.4171\n",
      "Epoch 12: loss improved from 1.45181 to 1.41754, saving model to saved_weights/saved_weights-12-1.4175.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.4175\n",
      "Epoch 13/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.3928\n",
      "Epoch 13: loss improved from 1.41754 to 1.39315, saving model to saved_weights/saved_weights-13-1.3931.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.3931\n",
      "Epoch 14/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.3624\n",
      "Epoch 14: loss improved from 1.39315 to 1.36250, saving model to saved_weights/saved_weights-14-1.3625.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.3625\n",
      "Epoch 15/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.3498\n",
      "Epoch 15: loss improved from 1.36250 to 1.34944, saving model to saved_weights/saved_weights-15-1.3494.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.3494\n",
      "Epoch 16/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.3183\n",
      "Epoch 16: loss improved from 1.34944 to 1.31801, saving model to saved_weights/saved_weights-16-1.3180.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.3180\n",
      "Epoch 17/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.2940\n",
      "Epoch 17: loss improved from 1.31801 to 1.29695, saving model to saved_weights/saved_weights-17-1.2969.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2969\n",
      "Epoch 18/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.2870\n",
      "Epoch 18: loss improved from 1.29695 to 1.28701, saving model to saved_weights/saved_weights-18-1.2870.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2870\n",
      "Epoch 19/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.2687\n",
      "Epoch 19: loss improved from 1.28701 to 1.26860, saving model to saved_weights/saved_weights-19-1.2686.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2686\n",
      "Epoch 20/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.2565\n",
      "Epoch 20: loss improved from 1.26860 to 1.25589, saving model to saved_weights/saved_weights-20-1.2559.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2559\n",
      "Epoch 21/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.2381\n",
      "Epoch 21: loss improved from 1.25589 to 1.23903, saving model to saved_weights/saved_weights-21-1.2390.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2390\n",
      "Epoch 22/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.2322\n",
      "Epoch 22: loss improved from 1.23903 to 1.23365, saving model to saved_weights/saved_weights-22-1.2336.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.2336\n",
      "Epoch 23/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.2185\n",
      "Epoch 23: loss improved from 1.23365 to 1.21901, saving model to saved_weights/saved_weights-23-1.2190.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2190\n",
      "Epoch 24/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.2107\n",
      "Epoch 24: loss improved from 1.21901 to 1.21058, saving model to saved_weights/saved_weights-24-1.2106.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.2106\n",
      "Epoch 25/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.1961\n",
      "Epoch 25: loss improved from 1.21058 to 1.19716, saving model to saved_weights/saved_weights-25-1.1972.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.1972\n",
      "Epoch 26/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.1867\n",
      "Epoch 26: loss improved from 1.19716 to 1.18737, saving model to saved_weights/saved_weights-26-1.1874.hdf5\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.1874\n",
      "Epoch 27/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1678\n",
      "Epoch 27: loss improved from 1.18737 to 1.16794, saving model to saved_weights/saved_weights-27-1.1679.hdf5\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.1679\n",
      "Epoch 28/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1600\n",
      "Epoch 28: loss improved from 1.16794 to 1.16079, saving model to saved_weights/saved_weights-28-1.1608.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1608\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1600\n",
      "Epoch 29: loss improved from 1.16079 to 1.16004, saving model to saved_weights/saved_weights-29-1.1600.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1600\n",
      "Epoch 30/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1472\n",
      "Epoch 30: loss improved from 1.16004 to 1.14811, saving model to saved_weights/saved_weights-30-1.1481.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1481\n",
      "Epoch 31/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1399\n",
      "Epoch 31: loss improved from 1.14811 to 1.14000, saving model to saved_weights/saved_weights-31-1.1400.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1400\n",
      "Epoch 32/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.1201\n",
      "Epoch 32: loss improved from 1.14000 to 1.12082, saving model to saved_weights/saved_weights-32-1.1208.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.1208\n",
      "Epoch 33/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.1214\n",
      "Epoch 33: loss did not improve from 1.12082\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1214\n",
      "Epoch 34/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1132\n",
      "Epoch 34: loss improved from 1.12082 to 1.11327, saving model to saved_weights/saved_weights-34-1.1133.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1133\n",
      "Epoch 35/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 1.1021\n",
      "Epoch 35: loss improved from 1.11327 to 1.10252, saving model to saved_weights/saved_weights-35-1.1025.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.1025\n",
      "Epoch 36/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0916\n",
      "Epoch 36: loss improved from 1.10252 to 1.09225, saving model to saved_weights/saved_weights-36-1.0922.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0922\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0907\n",
      "Epoch 37: loss improved from 1.09225 to 1.09067, saving model to saved_weights/saved_weights-37-1.0907.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.0907\n",
      "Epoch 38/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.0663\n",
      "Epoch 38: loss improved from 1.09067 to 1.06790, saving model to saved_weights/saved_weights-38-1.0679.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0679\n",
      "Epoch 39/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0659\n",
      "Epoch 39: loss improved from 1.06790 to 1.06740, saving model to saved_weights/saved_weights-39-1.0674.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0674\n",
      "Epoch 40/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.0664\n",
      "Epoch 40: loss improved from 1.06740 to 1.06691, saving model to saved_weights/saved_weights-40-1.0669.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0669\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0562\n",
      "Epoch 41: loss improved from 1.06691 to 1.05622, saving model to saved_weights/saved_weights-41-1.0562.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0562\n",
      "Epoch 42/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.0473\n",
      "Epoch 42: loss improved from 1.05622 to 1.04763, saving model to saved_weights/saved_weights-42-1.0476.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0476\n",
      "Epoch 43/50\n",
      "225/228 [============================>.] - ETA: 0s - loss: 1.0401\n",
      "Epoch 43: loss improved from 1.04763 to 1.04147, saving model to saved_weights/saved_weights-43-1.0415.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0415\n",
      "Epoch 44/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 1.0299\n",
      "Epoch 44: loss improved from 1.04147 to 1.03021, saving model to saved_weights/saved_weights-44-1.0302.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.0302\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0215\n",
      "Epoch 45: loss improved from 1.03021 to 1.02148, saving model to saved_weights/saved_weights-45-1.0215.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0215\n",
      "Epoch 46/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0264\n",
      "Epoch 46: loss did not improve from 1.02148\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 1.0274\n",
      "Epoch 47/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0171\n",
      "Epoch 47: loss improved from 1.02148 to 1.01669, saving model to saved_weights/saved_weights-47-1.0167.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.0167\n",
      "Epoch 48/50\n",
      "226/228 [============================>.] - ETA: 0s - loss: 1.0143\n",
      "Epoch 48: loss improved from 1.01669 to 1.01505, saving model to saved_weights/saved_weights-48-1.0151.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 1.0151\n",
      "Epoch 49/50\n",
      "227/228 [============================>.] - ETA: 0s - loss: 0.9971\n",
      "Epoch 49: loss improved from 1.01505 to 0.99762, saving model to saved_weights/saved_weights-49-0.9976.hdf5\n",
      "228/228 [==============================] - 3s 12ms/step - loss: 0.9976\n",
      "Epoch 50/50\n",
      "224/228 [============================>.] - ETA: 0s - loss: 0.9920\n",
      "Epoch 50: loss improved from 0.99762 to 0.99235, saving model to saved_weights/saved_weights-50-0.9923.hdf5\n",
      "228/228 [==============================] - 3s 11ms/step - loss: 0.9923\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('my_saved_weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "zB_wnrpwOOqP",
    "outputId": "7c451b8a-1ca1-44bb-e9cf-42653d3fde53"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "453Qe2yhOUt3"
   },
   "outputs": [],
   "source": [
    "#Generate characters \n",
    "#We must provide a sequence of seq_lenth as input to start the generation process\n",
    "\n",
    "#The prediction results is probabilities for each of the 48 characters at a specific\n",
    "#point in sequence. Let us pick the one with max probability and print it out.\n",
    "#Writing our own softmax function....\n",
    "\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0u80iP9Oi-Y",
    "outputId": "f6d14c76-63e0-4350-aa90-bf753de3681f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed for our text prediction: \"eavy, as big, and as fierce as his\n",
      "father. “give me another \"\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n",
    "\n",
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)\n",
    "\n",
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xiI1Su1H272",
    "outputId": "1c5f3e8d-482e-4431-d844-0cb5bfd271c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152586"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T86aP3zTOuZV",
    "outputId": "c3e7573e-56f1-48a0-9621-114af4d51d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "bround,\n",
      "but, and the beather when the monkeys gun and shoulder fater, him in a little parces and seephing, it smust he had"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-8e775d08c2e9>:10: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "never we rulled of the sturned up him the monkeys been tway to do.\n",
      "\n",
      "“that little balk not have been the\n",
      ", banglight!” said the camel run down to do ir wolves. but before what they call that yeur in the lames from buck have chind, and the troward. flicked of the beafs aulter\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char) # writing one character at a time \n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzAY3tWSOrij"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzuHmZZeIWWI"
   },
   "source": [
    "# New section"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
